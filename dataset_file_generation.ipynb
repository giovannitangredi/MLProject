{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GIT_dataset_file_generation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gDp-K4MIHHnL"},"source":["Dataset adaptation"]},{"cell_type":"markdown","metadata":{"id":"SZA9p3G13TaQ"},"source":["Use this cell only if you ned to connect to a google drive otherwise you can ignore it"]},{"cell_type":"code","metadata":{"id":"_bkSXLnGFGlc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622899209144,"user_tz":-120,"elapsed":25319,"user":{"displayName":"Giovanni Tangredi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1JXPw9z9lVuZWGZCKCrcINCDhtjYLCsSuObTv=s64","userId":"11189073661273032317"}},"outputId":"8c0d7a37-f63e-40d7-ef55-39006e834df7"},"source":["# connect to drive\n","import json\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/gdrive')\n","files_dir = \"/content/gdrive/My Drive/PRJ/{}\"\n","\n","base_file_dir = files_dir.format(\"\")\n","\n","# go to PRJ folder\n","%cd \"{base_file_dir}\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/.shortcut-targets-by-id/1qedjgXhTvPN8l-U_zLXcGXl9O7xge2q7/PRJ\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w-XHwlg-Jr4V"},"source":["# convert original files (json) to category files \n","\n","from torch.utils.data import Dataset\n","import math\n","\n","out_dir=\"converted\"\n","#out_dir=\"converted-val\"\n","categories_file=\"image_info_test-dev2017.json\"\n","instances_file=\"instances_train2017.json\"\n","#instances_file=\"instances_val2017.json\"\n","captions_file=\"captions_train2017.json\"\n","#captions_file=\"captions_val2017.json\"\n"," \n","file_ext = \".txt\"\n","labels = {} \n","\n","\n","print(\"Converting COCO data\")\n","categories_by_id = {} \n","images_categorized = {} \n","coco_categories = set()\n","\n","# get supercategories from json file\n","with open(files_dir.format(categories_file), \"r\") as f3:\n","  info = json.load(f3)\n","  for cat in info[\"categories\"]:\n","    key = \"supercategory\"\n","    categories_by_id[cat[\"id\"]] = cat[key]\n","    coco_categories.add(cat[key].lower())\n","  print(\"COCO Categories loaded\")\n","  print(\"COCO Categories are {}\".format(coco_categories))\n","\n","    \n","# get image ids divided by categories\n","with open(files_dir.format(instances_file), \"r\") as f2:\n","  instances = json.load(f2)\n","  for cat in instances[\"annotations\"]:\n","    if cat[\"image_id\"] in images_categorized:\n","      images_categorized[cat[\"image_id\"]].add(categories_by_id[cat[\"category_id\"]])\n","    else:\n","      images_categorized[cat[\"image_id\"]] = {categories_by_id[cat[\"category_id\"]]}\n","  print(\"Instances loaded\")\n","\n","# put captions in a dict {categ : captions}\n","with open(files_dir.format(captions_file), \"r\") as f:\n","  caption = json.load(f)\n","  for annotation in caption[\"annotations\"]:\n","    label = annotation[\"caption\"]\n","    image_id = annotation[\"image_id\"]\n","    if image_id in images_categorized:\n","      for cat in images_categorized[image_id]:\n","        if cat in labels:\n","          labels[cat].add(label)\n","        else:\n","          labels[cat] = {label} \n","  print(\"Captions loaded\")   \n","\n","# save all data in new files\n","if (not os.path.isdir(out_dir)):\n","  os.mkdir(out_dir)\n","\n","converted_files = {}\n","for cat in labels:\n","  out_file = out_dir+\"/\"+cat+file_ext\n","  with open(out_file,\"w\") as f:\n","    for data in labels[cat]:\n","      f.write(data.replace(\"\\n\",\"\")+\"\\n\")\n","    converted_files[out_file] = cat\n","print(\"Dataset generated.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wsMHC3AiFRgX"},"source":["# get all dataset (in a list) from already coverted file (divided in category)\n","\n","from torch.utils.data import Dataset\n","import math\n","import os\n","\n","def load_coco_data(in_dir):\n","    file_ext = \".txt\"\n","    labels = [] \n","    i = 0\n","    \n","    \n","    if os.path.isdir(in_dir):\n","      print(\"Adapated data directory found, will assume data has been already converted\") \n","\n","      # create dict {filename : category}\n","      files_categorized = {in_dir+\"/\"+f : f.replace(file_ext,\"\") for f in os.listdir(in_dir) if os.path.isfile(in_dir+\"/\"+f)}\n","\n","      # for all files\n","      for f in files_categorized:\n","        # get category\n","        category = files_categorized[f]\n","        #print(category)\n","        \n","        # open file and add all lines (with 'control code') to list\n","        with open(f,\"r\") as f1:\n","          for data in f1.readlines():\n","            labels.append(category.capitalize() + ' ' + data)\n","\n","    return labels\n","\n","\n","class CocoDataset(Dataset):\n","\n","  def __init__(self, in_dir):\n","    self.data = load_coco_data(in_dir)\n","    self.len = len(self.data)\n","\n","  def __len__(self):\n","    return self.len\n","\n","  def __getitem__(self, index):\n","    toRet = self.data[index]\n","    toRet = str(toRet)\n","    return toRet\n","\n","# load dataset class\n","dataset = CocoDataset(\"converted-val\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXgwtGV-K9BA"},"source":["# partitioning dataset\n","\n","import random \n","\n","def partition (list_in, n):\n","    return [list_in[i::n] for i in range(n)]\n","\n","# shuffle list to not have all category sentence near each other\n","for i in range(5):\n","  random.shuffle(dataset.data)\n","  \n","list_out = partition(dataset.data, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fV6L8QqtMpFG"},"source":["# create n files\n","\n","for i in range(n):\n","  f_name = \"file\"+str(i+1)+\".txt\"\n","  print(f_name)\n","  with open(f_name,\"w\") as f:\n","    for l in list_out[i]:\n","      f.write(l.replace(\"\\n\",\"\")+\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrpZfEbnpKwA"},"source":["Generation file for metrics"]},{"cell_type":"code","metadata":{"id":"ggTkpvSHo50c"},"source":["!pip install transformers\n","import os\n","import math\n","import gc\n","import json\n","from google.colab import drive\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import CTRLPreTrainedModel, CTRLConfig, CTRLTokenizer\n","from transformers.modeling_outputs import BaseModelOutputWithPast\n","from transformers.modeling_outputs import CausalLMOutputWithPast"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"neOP6mjFoY2W"},"source":["# setup CTRLEvolved model\n","from new_classes import CTRLLMHeadEvolvedModel\n","device = \"cuda\"\n","layers = 10\n","\n","print(\"Creating model...\")\n","model = CTRLLMHeadEvolvedModel(CTRLConfig(n_layer=layers, n_head=16))\n","model.to(device)\n","print(\"Model created.\")\n","\n","print(\"Loading model checkpoint...\")\n","model.load_state_dict(torch.load('./newModel/new_model.bin'))\n","print(\"Model checkpoint loaded\")\n","\n","tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n","tokenizer.add_special_tokens({'pad_token': '~'})\n","optimizer =torch.optim.Adagrad(model.parameters(), lr=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYpnL0_aQOUF"},"source":["!pip install transformers\n","\n","from transformers import CTRLConfig, CTRLLMHeadModel, CTRLTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Mw85ylTQBuE"},"source":["# setup standard model\n","\n","device = \"cuda\"\n","layers = 10\n","\n","print(\"Creating model...\")\n","model = CTRLLMHeadModel(CTRLConfig(n_layer=layers))\n","model = model.to(device)\n","print(\"Model created.\")\n","\n","print(\"Loading model checkpoint...\")\n","\n","# load partial trained model\n","model.load_state_dict(torch.load('./trained/model_from_scratch.bin'))\n","print(\"Model checkpoint loaded\")\n","\n","tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n","tokenizer.add_special_tokens({'pad_token': '~'})\n","optimizer =torch.optim.Adagrad(model.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPcHoO_upsTi"},"source":["# generate input list\n","import random \n","\n","reference_file_path = \"./references.txt\"\n","n_sentence = 1000\n","list_out = []\n","min_words = 3\n","max_words = 5\n","\n","# open file and get n_sentence examples randomly\n","with open(reference_file_path,\"r\") as f:\n","  tmp = f.readlines();\n","  random.shuffle(tmp)\n","  list_out = tmp[: n_sentence]\n","\n","\n","# take only some opening words\n","for i in range(n_sentence):\n","  random_index = random.randint(min_words, max_words)\n","  tmp = list_out[i].split(\" \")\n","  tmp = tmp[: random_index]\n","  list_out[i] = \" \".join(tmp)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-b_1pFaAHN8","executionInfo":{"status":"ok","timestamp":1622899370473,"user_tz":-120,"elapsed":229,"user":{"displayName":"Giovanni Tangredi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1JXPw9z9lVuZWGZCKCrcINCDhtjYLCsSuObTv=s64","userId":"11189073661273032317"}},"outputId":"23369749-2996-4dea-904b-3a50c5130a92"},"source":["len(list_out)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"SVMYP0wNprGn"},"source":["# generation of n_sentence\n","\n","# setup\n","n = n_sentence\n","list_input = list_out\n","candidate_list = []\n","\n","seq_length = 30\n","temperature = 1.0 #default=1.0\n","nucleusprob = 0.9 #default=0.9\n","penalty = 1.2     #help=\"primarily useful for CTRL model; in that case, use 1.2\"\n","topk = 0          #default=0\n","\n","\n","# start cycle\n","for i in range(n):\n","  prompt = list_input[i]\n","\n","  encoded_CTRL = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n","  encoded_input = encoded_CTRL.to(device)\n","\n","  len_prompt = len(encoded_input[0])\n","\n","  # generation of logits\n","  output_sequence = model.generate(\n","    input_ids=encoded_input,\n","    max_length= seq_length + len_prompt,\n","    temperature=temperature,\n","    top_k=topk,\n","    top_p=nucleusprob,\n","    repetition_penalty=penalty,\n","    do_sample=True,\n","    num_return_sequences=1,\n","  )\n","\n","  # get text from logits\n","  generated_sequence = output_sequence[0].tolist()\n","  text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n","\n","  # Remove all text after the stop token\n","  if (\".\" in text):\n","    text = text[: text.index(\".\")+1]\n","\n","  if (\"~\" in text):\n","    text = text[: text.index(\"~\")]\n","\n","  if (\"\\n\" in text):\n","    text = text[: text.index(\"\\n\")]\n","\n","  print(i)\n","\n","  # save text \n","  candidate_list.append(text)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQ9Ic4dGFHo-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622900368378,"user_tz":-120,"elapsed":16,"user":{"displayName":"Giovanni Tangredi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1JXPw9z9lVuZWGZCKCrcINCDhtjYLCsSuObTv=s64","userId":"11189073661273032317"}},"outputId":"992cc18c-cfda-4b8b-8fc6-b8742afaa922"},"source":["print(len(candidate_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A1PwzkiWx4SM"},"source":["# save candidate_list on file\n","\n","file_path = \"candidates/candidates.txt\"\n","\n","with open(file_path,\"w\") as f:\n","  for s in candidate_list:\n","    f.write(s + \"\\n\")"],"execution_count":null,"outputs":[]}]}