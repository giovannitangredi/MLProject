{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EvolvedCTRL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"s4lsqc-Lj1tE"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9h1lIR54N7PM"},"source":["!pip install transformers\n","\n","import os\n","import math\n","import gc\n","import json\n","from google.colab import drive\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import CTRLConfig, CTRLTokenizer, CTRLLMHeadModel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9gKfmAE1XOX"},"source":["Use this cell only if you ned to connect to a google drive otherwise you can ignore it"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RUxq_JN_Idy","executionInfo":{"status":"ok","timestamp":1622891984164,"user_tz":-120,"elapsed":20046,"user":{"displayName":"claudia manini","photoUrl":"","userId":"10921459249515493298"}},"outputId":"9b7f514a-8cc4-4c64-ad08-e772f8919ae6"},"source":["# connect to drive\n","\n","drive.mount('/content/gdrive')\n","files_dir = \"/content/gdrive/My Drive/PRJ/{}\"\n","\n","base_file_dir = files_dir.format(\"\")\n","\n","%cd \"{base_file_dir}\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/PRJ\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_RPqBnx7hp9C"},"source":["# setup model\n","from new_classes import CTRLLMHeadEvolvedModel\n","\n","device = \"cuda\"\n","layers = 10\n","print(\"Creating model...\")\n","# create evolved model\n","model = CTRLLMHeadEvolvedModel(CTRLConfig(n_layer=layers, n_head=16))\n","model.to(device)\n","print(\"Model created.\")\n","\n","print(\"Loading model checkpoint...\")\n","# load trained Evolved model if needed change file path\n","model.load_state_dict(torch.load('./newModel/new_model.bin'))\n","print(\"Model checkpoint loaded\")\n","\n","tokenizer = CTRLTokenizer.from_pretrained('ctrl')\n","tokenizer.add_special_tokens({'pad_token': '~'})\n","optimizer =torch.optim.Adagrad(model.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQnn2OC9iFhA"},"source":["# load train dataset (file name to be specified) \n","\n","def load_coco_data(file_path):\n","    labels = {} \n","    i = 0\n","    \n","    if os.path.isfile(file_path):\n","      print(\"Load from\", file_path)\n","      \n","      with open(file_path,\"r\") as f1:\n","        for sentence in f1.readlines():\n","          labels[i] = sentence\n","          i = i + 1\n","\n","      print(\"Dataset loaded\")\n","      return labels\n","    else:\n","      print(\"File not found\")\n","      return labels        \n","\n","\n","class CocoDataset(Dataset):\n","\n","  def __init__(self, file_path):\n","    self.data = load_coco_data(file_path)\n","    self.len = len(self.data.values())\n","\n","  def __len__(self):\n","    return self.len\n","\n","  def __getitem__(self, index):\n","    toRet = self.data[index]\n","    toRet = str(toRet)\n","    return toRet\n","\n","# load dataset from file path\n","dataset = CocoDataset(\"data_shuffle/file12.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNzcDXVeiMHz"},"source":["# Training\n","\n","data_loader = DataLoader(dataset)\n","\n","x=0\n","batch_s = 13\n","l = []\n","model.train()\n","\n","for i, data in enumerate(data_loader):\n","  \n","  # Step 1: Retrieving a batch of input from the dataloader\n","  \n","  l.append(data[0]) # accumulate batch_s sentences\n","\n","  if ((i%batch_s == 0 and i != 0) or i == (dataset.len - 1)): # reach batch_s -> do training step\n","    \n","    input = tokenizer(l, return_tensors='pt', padding = True, truncation=True).to(device)\n","    l.clear()\n","  else:\n","    continue # skip to next sentence    \n","  \n","  # Step 2: Zeroing the parameter gradients - always do this before doing loss.backward()!!!\n","  optimizer.zero_grad()\n","      \n","  # Step 3: forward (get network prediction)\n","  outputs = model(**input, labels = input[\"input_ids\"])\n","  \n","  # free GPU from input data\n","  del input\n","  torch.cuda.empty_cache()\n","  gc.collect()\n","\n","  # generation of logits\n","  logits = outputs.logits\n","  \n","  # Step 4: compute loss\n","  loss = outputs.loss\n","  \n","  # Step 5: Compute gradients for each of the model learnable parameters\n","  loss.backward()\n","  \n","  # Step 4: Update model parameters according to the gradients\n","  optimizer.step()\n","\n","  # print after n steps\n","  x = x + 1\n","  if (x%1000 == 0 and x != 0):\n","    print(i)\n","   \n","  \n","# End Training here\n","print(\"Training done\")\n","\n","#saving the model after training\n","print(\"Saving model...\")\n","torch.save(model.state_dict(), \"/content/gdrive/My Drive/PRJ/newModel/new_model.bin\")\n","print(\"Model saved\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0_IkhuTnQEb"},"source":["# generation with model already loaded\n","print('INFO :: Start!!') \n","\n","# setup\n","seq_length = 30\n","temperature = 1.0 #default=1.0\n","nucleusprob = 0.9 #default=0.9\n","penalty = 1.2     #help=\"primarily useful for CTRL model; in that case, use 1.2\"\n","topk = 0          #default=0\n","\n","# get prompt\n","# Vehicle A man sitting on his motorcycle with one hand on his helmet. \n","# Food Several dishes contain a wide variety of vegetables.\n","# Outdoor Two women walk across a street-crossing in a city with cars and a bus driving by.\n","# Person A man stands barefoot on a sandy beach.\n","# Electronic A boy using his phone and fanning himself.\n","prompt = input('ENTER PROMPT: ')\n","\n","encoded_CTRL = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n","encoded_input = encoded_CTRL.to(device)\n","\n","len_prompt = len(encoded_input[0])\n","\n","# generation of logits\n","output_sequence = model.generate(\n","  input_ids=encoded_input,\n","  max_length= seq_length + len_prompt,\n","  temperature=temperature,\n","  top_k=topk,\n","  top_p=nucleusprob,\n","  repetition_penalty=penalty,\n","  do_sample=True,\n","  num_return_sequences=1,\n",")\n","\n","# get text from logits\n","generated_sequence = output_sequence[0].tolist()\n","text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n","\n","# Remove all text after the stop token\n","if (\".\" in text):\n","  text = text[: text.index(\".\")+1]\n","\n","if (\"\\n\" in text):\n","  text = text[: text.index(\"\\n\")]\n","\n","print(\"=== GENERATED SEQUENCE ===\")\n","print(text)\n","\n","print('INFO :: Generation done!')"],"execution_count":null,"outputs":[]}]}